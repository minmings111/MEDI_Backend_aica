Redis 필터링 결과 저장 전략 (Best Practice)

필터링 에이전트의 목표는 youtube:video:{id}:comments (원본 댓글)을 읽고, LLM의 분류 결과를 효율적으로 저장하여 나중에 빠르게 조회할 수 있도록 하는 것입니다.

1. 왜 원본 JSON을 수정하면 안 되나요?

제안하신 "메타데이터 형식에 마지막에 변수만 추가"하는 방식은 다음과 같은 치명적인 단점이 있습니다.

Read-Modify-Write (읽기-수정-쓰기)의 비효율:

저장하려면, 에이전트는 Redis에서 거대한 원본 JSON 문자열을 GET해야 합니다.

JSON을 파싱(Parse)합니다.

모든 댓글과 대댓글을 일일이 찾아다니며 classification: "normal" 같은 새 필드를 추가합니다.

수정된 거대 JSON을 다시 문자열로 SET합니다.

이 작업은 댓글이 100개만 되어도 매우 느리고 CPU를 많이 소모합니다.

데이터 오염 (Data Contamination):

원본 데이터(raw_comments)와 처리된 데이터(classification)가 한 파일에 섞이게 됩니다.

만약 LLM 모델을 V2로 업그레이드해서 재분류하고 싶을 때, 이미 V1 결과가 섞인 JSON을 또 읽어서 덮어써야 하는 등 관리가 복잡해집니다.

조회(Querying)의 비효율:

"이 영상에서 필터링된 댓글 5개만 보여줘"라는 요청을 처리하려면, 매번 거대한 원본 JSON을 GET해서 전체를 파싱한 뒤, classification == "filtered"인 항목을 찾아내야 합니다. 이는 데이터베이스의 재앙입니다.

2. 권장: Hash를 사용한 '분류 조회 테이블'

가장 효율적인 방법은 원본 데이터는 절대 건드리지 않고, 분류 결과만 별도의 Hash 키에 저장하는 것입니다.

원본 데이터 키 (Java가 저장):

youtube:video:PLo9u5Mm5TA:comments

(Type: String) - (Value: 거대한 댓글 JSON)

분류 결과 키 (AI가 저장):

video:PLo9u5Mm5TA:classification

(Type: Hash)

이 Hash 키 내부는 다음과 같이 구성됩니다.

Field (필드)

Value (값)

UgyQbxEOl74hRc1xkQl4AaABAg

{"status": "normal"}

ADFjYOKUh-sADa-V9q-trq

{"status": "normal"}

UgxIiFkVYl5xvgnTmhx4AaABAg

{"status": "normal"}

UgyQ79ye0h-o-lNecgN4AaABAg

{"status": "content_suggestion"}

UgwBAybOcuLCKN6ZqEJ4AaABAg

{"status": "normal"}

ADHOQLr3yMjADHP8FWsV0r

{"status": "normal"}

(만약 악성 댓글이 있다면)

{"status": "filtered", "reason": "hate_speech"}

_metadata

{"updated_at": "...", "model_version": "v1.2"}

3. 실제 에이전트의 작업 흐름 (Workflow)

[데이터 로드]

에이전트가 youtube:video:PLo9u5Mm5TA:comments 키에서 원본 JSON을 **GET**합니다.

JSON을 파싱하여 모든 comment_id(부모, 자식 포함)와 textOriginal을 리스트로 추출합니다.

[LLM 호출]

이 리스트를 LLM에 전달하여 {"normal": [...], "filtered": [...], "content_suggestion": [...]} 결과를 받습니다.

[분류 결과 저장 (가장 중요)]

에이전트는 이 결과를 바탕으로 Python 딕셔너리를 만듭니다.

classification_map = {}
for comment_id in result['normal']:
    classification_map[comment_id] = json.dumps({"status": "normal"})

for comment_id in result['filtered']:
    classification_map[comment_id] = json.dumps({"status": "filtered", "reason": "auto_detected"})

for comment_id in result['content_suggestion']:
     classification_map[comment_id] = json.dumps({"status": "content_suggestion"})

# ... 메타데이터 추가
classification_map["_metadata"] = json.dumps({"updated_at": "...", "model": "gpt-5-mini"})


이 맵을 단 한 번의 HSET 명령으로 Redis에 저장합니다.

# HSET video:PLo9u5Mm5TA:classification MAPPING {classification_map}
r.hset(name="video:PLo9u5Mm5TA:classification", mapping=classification_map)