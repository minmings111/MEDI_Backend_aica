from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional

from googleapiclient.errors import HttpError

import config
from api_manager import APIKeyManager
from data_processor import VideoDataProcessor


class ChannelCommentFetcher:
    """ê¸°ì¡´ ì˜ìƒ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ëŠ” ìœ í‹¸ë¦¬í‹°."""

    def __init__(
        self,
        api_manager: APIKeyManager,
        output_root: Path,
        *,
        max_comments_per_video: Optional[int],
        page_size: int,
        order: str,
        text_format: str = "html",
    ):
        if page_size < 1 or page_size > 100:
            raise ValueError("page_size ê°’ì€ 1~100 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        self.api_manager = api_manager
        self.processor = VideoDataProcessor()
        self.output_root = output_root
        self.output_root.mkdir(parents=True, exist_ok=True)

        self.max_comments_per_video = max_comments_per_video
        self.page_size = page_size
        self.order = order
        self.text_format = text_format

    # --------------------------------------------------------------------- #
    # ë¹„ë””ì˜¤ ID ë¡œë“œ
    # --------------------------------------------------------------------- #
    def _load_video_data(self, channel_id: str) -> Dict[str, Any]:
        """ì±„ë„ì˜ ì˜ìƒ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

        ìš°ì„ ìˆœìœ„:
        1. `output/video_data/videos_{channel_id}.json`
        2. `output/channel_highlights/**/all_videos_{channel_id}.json`
        """
        video_data_dir = config.Config.OUTPUT_DIR / "video_data"
        video_data_file = video_data_dir / f"videos_{channel_id}.json"

        if video_data_file.exists():
            with open(video_data_file, "r", encoding="utf-8") as f:
                return json.load(f)

        highlights_root = config.Config.OUTPUT_DIR / "channel_highlights"
        if highlights_root.exists():
            pattern = f"all_videos_{channel_id}.json"
            for candidate in highlights_root.glob(f"**/{pattern}"):
                with open(candidate, "r", encoding="utf-8") as f:
                    data = json.load(f)
                print(f"â„¹ï¸  ì±„ë„ í•˜ì´ë¼ì´íŠ¸ ë°ì´í„° ì‚¬ìš©: {candidate}")
                return data

        raise FileNotFoundError(
            f"ì±„ë„ ID {channel_id}ì— ëŒ€í•œ ì˜ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            "ë¨¼ì € ì˜ìƒ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œ ë’¤ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”."
        )

    def _extract_video_ids(self, video_data: Dict[str, Any]) -> List[str]:
        videos = video_data.get("videos", [])
        video_ids = []

        for video in videos:
            video_id = video.get("video_id") or video.get("id")
            if video_id:
                video_ids.append(video_id)

        if not video_ids:
            raise ValueError("ì˜ìƒ ë°ì´í„°ì—ì„œ video_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        return video_ids

    # --------------------------------------------------------------------- #
    # ëŒ“ê¸€ ìˆ˜ì§‘
    # --------------------------------------------------------------------- #
    def fetch_comments_for_video(self, video_id: str) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ ì˜ìƒì˜ ëŒ“ê¸€ì„ ëª¨ë‘ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        collected: List[Dict[str, Any]] = []
        next_page_token: Optional[str] = None
        units_used = 0

        try:
            while True:
                if (
                    self.max_comments_per_video is not None
                    and len(collected) >= self.max_comments_per_video
                ):
                    break

                max_results = self.page_size
                if self.max_comments_per_video is not None:
                    remaining = self.max_comments_per_video - len(collected)
                    if remaining <= 0:
                        break
                    max_results = min(max_results, remaining)

                request = self.api_manager.youtube_service.commentThreads().list(
                    part="snippet,replies",
                    videoId=video_id,
                    maxResults=max_results,
                    pageToken=next_page_token,
                    order=self.order,
                    textFormat=self.text_format,
                )

                response = self.api_manager.execute_request(request.execute)
                units_used += 2  # commentThreads().list í˜¸ì¶œ ë¹„ìš©

                items = response.get("items", [])
                if not items:
                    break

                collected.extend(items)
                next_page_token = response.get("nextPageToken")

                if not next_page_token:
                    break

        except HttpError as error:
            reason = self._extract_error_reason(error)
            if reason in {"commentsDisabled", "disabledComments"}:
                print(f"   â„¹ï¸  ëŒ“ê¸€ì´ ë¹„í™œì„±í™”ëœ ì˜ìƒì…ë‹ˆë‹¤: {video_id}")
                collected = []
            else:
                print(f"   âš ï¸  ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ({video_id}): {error}")
                collected = []
        except Exception as error:  # pylint: disable=broad-except
            print(f"   âš ï¸  ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜ ({video_id}): {error}")
            collected = []
        finally:
            if units_used:
                self.api_manager.record_usage(units_used)

        return collected

    @staticmethod
    def _extract_error_reason(error: HttpError) -> str:
        try:
            data = json.loads(error.content.decode("utf-8"))
            errors = data.get("error", {}).get("errors", [])
            if errors:
                return errors[0].get("reason", "")
        except Exception:  # pylint: disable=broad-except
            pass
        return ""

    # --------------------------------------------------------------------- #
    # ë©”ì¸ ì²˜ë¦¬
    # --------------------------------------------------------------------- #
    def process_channel(
        self,
        channel_id: str,
        *,
        force: bool = False,
        limit_videos: Optional[int] = None,
    ) -> Dict[str, List[Dict[str, Any]]]:
        """ì§€ì •ëœ ì±„ë„ì˜ ëª¨ë“  ì˜ìƒ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤."""
        video_data = self._load_video_data(channel_id)
        video_ids = self._extract_video_ids(video_data)

        if limit_videos is not None:
            video_ids = video_ids[:limit_videos]

        output_dir = self.output_root / channel_id
        output_dir.mkdir(parents=True, exist_ok=True)

        aggregated_path = (
            config.Config.OUTPUT_DIR
            / "comment_data"
            / f"comments_{channel_id}.json"
        )
        aggregated_path.parent.mkdir(parents=True, exist_ok=True)

        aggregated_data = self._load_existing_aggregated(aggregated_path)

        updated_comments: Dict[str, List[Dict[str, Any]]] = {}

        print(f"ğŸ¯ ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘ - ì±„ë„: {channel_id}, ì˜ìƒ {len(video_ids)}ê°œ")

        for idx, video_id in enumerate(video_ids, start=1):
            per_video_path = output_dir / f"comments_{video_id}.json"
            if per_video_path.exists() and not force:
                print(
                    f"[{idx}/{len(video_ids)}] â­ï¸  ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ëŒ“ê¸€ íŒŒì¼ ê±´ë„ˆëœ€: {video_id}"
                )
                continue

            print(f"[{idx}/{len(video_ids)}] ğŸ’¬ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘: {video_id}")
            threads = self.fetch_comments_for_video(video_id)
            if not threads:
                print(f"   âš ï¸  ëŒ“ê¸€ì´ ì—†ê±°ë‚˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {video_id}")
                continue

            comments = [
                self.processor.extract_comment_info(thread) for thread in threads
            ]

            self._save_per_video(per_video_path, channel_id, video_id, comments)
            updated_comments[video_id] = comments
            aggregated_data.setdefault("comments", {})[video_id] = comments

        if updated_comments:
            aggregated_data["channel_id"] = channel_id
            self._save_aggregated(aggregated_path, aggregated_data)
            print(f"âœ… ëŒ“ê¸€ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {aggregated_path}")
        else:
            print("â„¹ï¸  ìƒˆë¡œ ì €ì¥ëœ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")

        return updated_comments

    @staticmethod
    def _load_existing_aggregated(path: Path) -> Dict[str, Any]:
        if path.exists():
            try:
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except json.JSONDecodeError:
                print(f"âš ï¸  ê¸°ì¡´ ëŒ“ê¸€ íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤: {path}")
        return {"channel_id": "", "comments": {}}

    @staticmethod
    def _save_per_video(
        path: Path,
        channel_id: str,
        video_id: str,
        comments: List[Dict[str, Any]],
    ):
        data = {
            "channel_id": channel_id,
            "video_id": video_id,
            "comments": comments,
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"   ğŸ’¾ ì €ì¥ ì™„ë£Œ: {path}")

    @staticmethod
    def _save_aggregated(path: Path, data: Dict[str, Any]):
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)


def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="ê¸°ì¡´ ì˜ìƒ ì •ë³´ë¥¼ ì´ìš©í•´ YouTube ëŒ“ê¸€ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--channel-id",
        required=True,
        help="ëŒ“ê¸€ì„ ìˆ˜ì§‘í•  ì±„ë„ ID (ì˜ˆ: UCxxxxxxxxxxxxxx)",
    )
    parser.add_argument(
        "--max-comments",
        type=int,
        default=None,
        help="ì˜ìƒë‹¹ ìµœëŒ€ ëŒ“ê¸€ ìˆ˜ (ê¸°ë³¸ê°’: ì œí•œ ì—†ìŒ)",
    )
    parser.add_argument(
        "--page-size",
        type=int,
        default=100,
        help="API ìš”ì²­ë‹¹ ê°€ì ¸ì˜¬ ìµœëŒ€ ëŒ“ê¸€ ìˆ˜ (1~100, ê¸°ë³¸ê°’ 100)",
    )
    parser.add_argument(
        "--order",
        choices=("relevance", "time"),
        default="relevance",
        help="ëŒ“ê¸€ ì •ë ¬ ê¸°ì¤€ (relevance | time)",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ëŒ“ê¸€ íŒŒì¼ì´ ìˆì–´ë„ ë‹¤ì‹œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--limit-videos",
        type=int,
        default=None,
        help="ì•ì—ì„œë¶€í„° Nê°œì˜ ì˜ìƒì— ëŒ€í•´ì„œë§Œ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=config.Config.OUTPUT_DIR / "comment_data_by_video",
        help="ëŒ“ê¸€ JSONì„ ì €ì¥í•  ë£¨íŠ¸ ë””ë ‰í† ë¦¬",
    )
    return parser.parse_args(argv)


def main(argv: Optional[List[str]] = None):
    args = parse_args(argv)

    config.Config.ensure_directories()

    try:
        api_keys = config.Config.load_api_keys()
    except Exception as error:  # pylint: disable=broad-except
        print(f"âŒ API í‚¤ ë¡œë“œ ì‹¤íŒ¨: {error}")
        return 1

    api_manager = APIKeyManager(api_keys)

    fetcher = ChannelCommentFetcher(
        api_manager,
        output_root=args.output_dir,
        max_comments_per_video=args.max_comments,
        page_size=args.page_size,
        order=args.order,
    )

    try:
        fetcher.process_channel(
            args.channel_id,
            force=args.force,
            limit_videos=args.limit_videos,
        )
    except FileNotFoundError as error:
        print(f"âŒ {error}")
        print("ğŸ“Œ ë¨¼ì € ì˜ìƒ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œ ë’¤ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return 1
    except Exception as error:  # pylint: disable=broad-except
        print(f"âŒ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error}")
        return 1

    print("ğŸ‰ ëŒ“ê¸€ ìˆ˜ì§‘ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
    return 0


if __name__ == "__main__":
    sys.exit(main())


